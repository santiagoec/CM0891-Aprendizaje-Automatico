{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "Q-Learning MDP_Version_Actual.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santiagoec/CM0891-Aprendizaje-Automatico/blob/master/04_Aprendizaje_Reforzado_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo7eMo94f3c1",
        "colab_type": "text"
      },
      "source": [
        "# Aprendizaje Reforzado en Python\n",
        "\n",
        "Este libro contiene un estudio de **Aprendizaje Reforzado** utilizando Python. Esto hace parte del curso de Aprendizaje Automático de la Maestría de Ciencia de Datos.  Consiste en la implementación del algoritmo Q-learning en un Proceso de Decisión de Markov\n",
        "\n",
        "Los integrantes de este trabajo:\n",
        "\n",
        "* Santiago Echeverri Calderon\n",
        "* Edgar Leandro Jimenez Jaimes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnfQ6Qybf3c6",
        "colab_type": "text"
      },
      "source": [
        "Basado en el artículo de Venelin Valkov  \n",
        "https://medium.com/@curiousily/solving-an-mdp-with-q-learning-from-scratch-deep-reinforcement-learning-for-hackers-part-1-45d1d360c120"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25FnfH7Nf3c-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from copy import deepcopy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG4YgEp0aCWJ",
        "colab_type": "text"
      },
      "source": [
        "## 1. Creación del entorno"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCo-kiNCf3dN",
        "colab_type": "text"
      },
      "source": [
        "Creación de una cuadricula para un proceso de decisión de Markov simple con 16 estados posibles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGesZzWpf3dP",
        "colab_type": "code",
        "outputId": "879d9c50-6733-41d3-ba9b-42595da69c56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Defición de las variables\n",
        "\n",
        "OBSTACULO = \"o\"\n",
        "ROBOT = \"r\"\n",
        "TESORO = \"t\"\n",
        "VACIO = \"*\"\n",
        "\n",
        "# Definición del entorno o cuadricula\n",
        "grid = [\n",
        "    [ROBOT, VACIO, VACIO, OBSTACULO],\n",
        "    [VACIO, OBSTACULO,VACIO, VACIO],\n",
        "    [VACIO,VACIO,VACIO,OBSTACULO],\n",
        "    [VACIO,OBSTACULO,VACIO,TESORO]\n",
        "]\n",
        "\n",
        "\n",
        "for row in grid:\n",
        "    print(' '.join(row))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r * * o\n",
            "* o * *\n",
            "* * * o\n",
            "* o * t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU96PDpkf3dW",
        "colab_type": "text"
      },
      "source": [
        "Definición de una clase para poder acceder al estado y poder conocer la posición del robot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phUrAuL0f3da",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definición de clases \n",
        "class State:\n",
        "    \n",
        "    def __init__(self, grid, robot_pos):\n",
        "        self.grid = grid\n",
        "        self.robot_pos = robot_pos\n",
        "        \n",
        "    def __eq__(self, other):\n",
        "        return isinstance(other, State) and self.grid == other.grid and self.robot_pos == other.robot_pos\n",
        "    \n",
        "    def __hash__(self):\n",
        "        return hash(str(self.grid) + str(self.robot_pos))\n",
        "    \n",
        "    def __str__(self):\n",
        "        return f\"State(grid={self.grid}, robot_pos={self.robot_pos})\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV2EQKf2f3df",
        "colab_type": "text"
      },
      "source": [
        "El agente puede realizar 4 acciones: SUBIR, BAJAR, IR A LA IZQUIERDA, IR A LA DERECHA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjHPskesf3dh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definición del las acciones y asignaciones variables\n",
        "\n",
        "UP = 0\n",
        "DOWN = 1\n",
        "LEFT = 2\n",
        "RIGHT = 3\n",
        "\n",
        "ACTIONS = [UP, DOWN, LEFT, RIGHT]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRHaeVpxf3dp",
        "colab_type": "text"
      },
      "source": [
        "Definir y guardar la posicón inicial:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KlxjnJ4f3ds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_state = State(grid=grid, robot_pos=[0, 0]) # Posicion inicial donde se encuentra el robot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO6B8gHhWMY-",
        "colab_type": "text"
      },
      "source": [
        "Función para ejecutar las acciones (movimiento)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNN0boMif3d2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Funcion para el ejecutar las acciones (movimiento) y exploración\n",
        "\n",
        "def act(state, action):\n",
        "    \n",
        "    def new_robot_pos(state, action):\n",
        "        p = deepcopy(state.robot_pos)\n",
        "        if action == UP:\n",
        "            p[0] = max(0, p[0] - 1)\n",
        "        elif action == DOWN:\n",
        "            p[0] = min(len(state.grid) - 1, p[0] + 1)\n",
        "        elif action == LEFT:\n",
        "            p[1] = max(0, p[1] - 1)\n",
        "        elif action == RIGHT:\n",
        "            p[1] = min(len(state.grid[0]) - 1, p[1] + 1)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown action {action}\")\n",
        "        return p\n",
        "            \n",
        "    p = new_robot_pos(state, action)\n",
        "    grid_item = state.grid[p[0]][p[1]]\n",
        "    \n",
        "    new_grid = deepcopy(state.grid)\n",
        "    \n",
        "    if grid_item == OBSTACULO:\n",
        "        reward = -100\n",
        "        is_done = True\n",
        "        new_grid[p[0]][p[1]] += ROBOT\n",
        "    elif grid_item == TESORO:\n",
        "        reward = 1000\n",
        "        is_done = True\n",
        "        new_grid[p[0]][p[1]] += ROBOT\n",
        "    elif grid_item == VACIO:\n",
        "        reward = -1\n",
        "        is_done = False\n",
        "        old = state.robot_pos\n",
        "        new_grid[old[0]][old[1]] = VACIO\n",
        "        new_grid[p[0]][p[1]] = ROBOT\n",
        "    elif grid_item == ROBOT:\n",
        "        reward = -1\n",
        "        is_done = False\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown grid item {grid_item}\")\n",
        "    \n",
        "    return State(grid=new_grid, robot_pos=p), reward, is_done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pnx2oy5WWhb",
        "colab_type": "text"
      },
      "source": [
        "## 2. Entrenamiento de modelo (exploración del entorno)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIBXmIISf3eA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parámetros de la exploración\n",
        "\n",
        "random.seed(42)                                  # Semilla para aleatorios \n",
        "\n",
        "# Definición de pasos a seguir\n",
        "\n",
        "N_STATES = 16                                    # Posibles estados del entorno\n",
        "N_EPISODES = 100                                   # Ajustamos los episodios de exploracion\n",
        "MAX_EPISODE_STEPS = 100                          # Número máximo de pasos por episodio\n",
        "MIN_ALPHA = 0.02                                 # Valor mínimo de la tasa de aprendizaje\n",
        "alphas = np.linspace(1.0, MIN_ALPHA, N_EPISODES) # Array decreciente de valores alfa para cada episodio\n",
        "gamma = 1.0\n",
        "eps = 0.2                                        # epsilon de balance entre exploración y explotación\n",
        "q_table = dict()                                 # Diccionario para almacenar la tabla 'q'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akQQ_r0Xf3eM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Función para calcular el valor 'q' de cada acción para un estado dado\n",
        "\n",
        "def q(state, action=None):\n",
        "    \n",
        "    if state not in q_table:\n",
        "        q_table[state] = np.zeros(len(ACTIONS))\n",
        "        \n",
        "    if action is None:\n",
        "        return q_table[state]\n",
        "    \n",
        "    return q_table[state][action]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq9yVJ1_f3eW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Función para definir que accion tomar\n",
        "\n",
        "def choose_action(state):\n",
        "    if random.uniform(0, 1) < eps:\n",
        "        return random.choice(ACTIONS) \n",
        "    else:\n",
        "        return np.argmax(q(state))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtPzKCnuf3ef",
        "colab_type": "code",
        "outputId": "8743a49c-1f9c-447d-a433-72defdb2ebe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Exploración del entorno según los parámetros definidos\n",
        "\n",
        "for e in range(N_EPISODES):\n",
        "    \n",
        "    state = start_state\n",
        "    total_reward = 0\n",
        "    alpha = alphas[e]\n",
        "    \n",
        "    for _ in range(MAX_EPISODE_STEPS):\n",
        "        action = choose_action(state)\n",
        "        next_state, reward, done = act(state, action)\n",
        "        total_reward += reward\n",
        "        \n",
        "        q(state)[action] = q(state, action) + \\\n",
        "                alpha * (reward + gamma *  np.max(q(next_state)) - q(state, action))\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "    print(f\"Episode {e + 1}: total reward -> {total_reward}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 1: total reward -> -113\n",
            "Episode 2: total reward -> -104\n",
            "Episode 3: total reward -> -102\n",
            "Episode 4: total reward -> -108\n",
            "Episode 5: total reward -> -101\n",
            "Episode 6: total reward -> -107\n",
            "Episode 7: total reward -> -107\n",
            "Episode 8: total reward -> -101\n",
            "Episode 9: total reward -> -111\n",
            "Episode 10: total reward -> -104\n",
            "Episode 11: total reward -> -106\n",
            "Episode 12: total reward -> -105\n",
            "Episode 13: total reward -> -106\n",
            "Episode 14: total reward -> -108\n",
            "Episode 15: total reward -> -115\n",
            "Episode 16: total reward -> -108\n",
            "Episode 17: total reward -> 995\n",
            "Episode 18: total reward -> 989\n",
            "Episode 19: total reward -> -112\n",
            "Episode 20: total reward -> -103\n",
            "Episode 21: total reward -> -111\n",
            "Episode 22: total reward -> 990\n",
            "Episode 23: total reward -> 985\n",
            "Episode 24: total reward -> 994\n",
            "Episode 25: total reward -> 995\n",
            "Episode 26: total reward -> -105\n",
            "Episode 27: total reward -> -103\n",
            "Episode 28: total reward -> -103\n",
            "Episode 29: total reward -> -101\n",
            "Episode 30: total reward -> -103\n",
            "Episode 31: total reward -> 995\n",
            "Episode 32: total reward -> -102\n",
            "Episode 33: total reward -> 993\n",
            "Episode 34: total reward -> -107\n",
            "Episode 35: total reward -> 995\n",
            "Episode 36: total reward -> 994\n",
            "Episode 37: total reward -> 992\n",
            "Episode 38: total reward -> 995\n",
            "Episode 39: total reward -> 993\n",
            "Episode 40: total reward -> 991\n",
            "Episode 41: total reward -> -101\n",
            "Episode 42: total reward -> 991\n",
            "Episode 43: total reward -> 995\n",
            "Episode 44: total reward -> 994\n",
            "Episode 45: total reward -> 995\n",
            "Episode 46: total reward -> -106\n",
            "Episode 47: total reward -> 995\n",
            "Episode 48: total reward -> 994\n",
            "Episode 49: total reward -> 995\n",
            "Episode 50: total reward -> 995\n",
            "Episode 51: total reward -> 995\n",
            "Episode 52: total reward -> 995\n",
            "Episode 53: total reward -> 993\n",
            "Episode 54: total reward -> -104\n",
            "Episode 55: total reward -> -102\n",
            "Episode 56: total reward -> 993\n",
            "Episode 57: total reward -> 995\n",
            "Episode 58: total reward -> 995\n",
            "Episode 59: total reward -> 995\n",
            "Episode 60: total reward -> -102\n",
            "Episode 61: total reward -> 993\n",
            "Episode 62: total reward -> -104\n",
            "Episode 63: total reward -> 991\n",
            "Episode 64: total reward -> 995\n",
            "Episode 65: total reward -> -108\n",
            "Episode 66: total reward -> 994\n",
            "Episode 67: total reward -> -103\n",
            "Episode 68: total reward -> 995\n",
            "Episode 69: total reward -> 995\n",
            "Episode 70: total reward -> 993\n",
            "Episode 71: total reward -> 994\n",
            "Episode 72: total reward -> 995\n",
            "Episode 73: total reward -> 995\n",
            "Episode 74: total reward -> 995\n",
            "Episode 75: total reward -> 995\n",
            "Episode 76: total reward -> -104\n",
            "Episode 77: total reward -> 995\n",
            "Episode 78: total reward -> 993\n",
            "Episode 79: total reward -> -112\n",
            "Episode 80: total reward -> 995\n",
            "Episode 81: total reward -> 995\n",
            "Episode 82: total reward -> 994\n",
            "Episode 83: total reward -> 995\n",
            "Episode 84: total reward -> 995\n",
            "Episode 85: total reward -> 995\n",
            "Episode 86: total reward -> -101\n",
            "Episode 87: total reward -> 995\n",
            "Episode 88: total reward -> 995\n",
            "Episode 89: total reward -> 995\n",
            "Episode 90: total reward -> 994\n",
            "Episode 91: total reward -> -102\n",
            "Episode 92: total reward -> 993\n",
            "Episode 93: total reward -> 995\n",
            "Episode 94: total reward -> 995\n",
            "Episode 95: total reward -> 991\n",
            "Episode 96: total reward -> -101\n",
            "Episode 97: total reward -> -102\n",
            "Episode 98: total reward -> -102\n",
            "Episode 99: total reward -> 995\n",
            "Episode 100: total reward -> 995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTl2RNTRZ5Jf",
        "colab_type": "text"
      },
      "source": [
        "## 3. Prueba del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gIZN6LPaf9g",
        "colab_type": "text"
      },
      "source": [
        "Recordemos el estado inicial y la posicón del robot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTuYRZvRaghP",
        "colab_type": "code",
        "outputId": "5232003b-b349-41ec-b876-889a99977285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "for row in start_state.grid:\n",
        "    print(' '.join(row))\n",
        "\n",
        "print('Posición inicial del robot: ', start_state.robot_pos)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r * * o\n",
            "* o * *\n",
            "* * * o\n",
            "* o * t\n",
            "Posición inicial del robot:  [0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGedhWWkXnfK",
        "colab_type": "text"
      },
      "source": [
        "Calculemos los valores *q* para cada posible acción en el estado inicial\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEOvQFO5f3ep",
        "colab_type": "code",
        "outputId": "3568b442-5d97-4767-9b4d-57fcade22175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "r = q(start_state)\n",
        "print(f\"up={r[UP]}, down={r[DOWN]}, left={r[LEFT]}, right={r[RIGHT]}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "up=792.9452455350621, down=211.5794085743094, left=968.1105568278055, right=994.9999999820002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5v77egdX4My",
        "colab_type": "text"
      },
      "source": [
        "En el resultado anterior se observa que la acción de mayor valor *q* es **RIGHT**, lo que significa que esta debería ser la acción a ejecutar pues es la que maximiza la recompensa\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_RKNPDrbAm7",
        "colab_type": "text"
      },
      "source": [
        "Ejecutemos ahora esa acción, y veamos en donde quedó el robot, cuál fue la recompensa de la acción y si ya finalizó la tarea:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JyX5_25YM2W",
        "colab_type": "code",
        "outputId": "3430a845-fb70-456a-8f54-c7424fc30c62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "new_state, reward, done = act(start_state, RIGHT)\n",
        "\n",
        "print(\"Nueva cuadricula del entorno: \")\n",
        "\n",
        "for row in new_state.grid:\n",
        "    print(' '.join(row))\n",
        "\n",
        "print('Posición del robot: ', new_state.robot_pos)\n",
        "print('Recompensa de la acción: ', reward)\n",
        "print('¿Tarea terminada?: ', done)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nueva cuadricula del entorno: \n",
            "* r * o\n",
            "* o * *\n",
            "* * * o\n",
            "* o * t\n",
            "Posición del robot:  [0, 1]\n",
            "Recompensa de la acción:  -1\n",
            "¿Tarea terminada?:  False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0UHoRlcbc3m",
        "colab_type": "text"
      },
      "source": [
        "En la cuadricula se puede observar que el robot se desplazó a la derecha, quedando en la posición (0,1).  Como llegó a una cuadricula vacía su recompensa fue -1.  Y como no ha llegado al tesoro, ni se chocó con un obstáculo, la tarea aún no termina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaTDja93b9yX",
        "colab_type": "text"
      },
      "source": [
        "Automaticemos ahora el proceso de desiciones del robot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cn8VLjUUjsw5",
        "colab_type": "code",
        "outputId": "4609188e-edc5-4429-9313-53e4d00d97bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "max_iter = 100                  # Definición de maximas iteraciones\n",
        "finished = False                # Parametro de parada\n",
        "actual_state = start_state      # Estado incial \n",
        "i = 0                           # Contador\n",
        "\n",
        "while  (i != max_iter or finished == True):\n",
        "  # Escoger la accion segun el estado\n",
        "  action = choose_action(actual_state)\n",
        "  # Ejecutar la accion definda y calcular la recompensa\n",
        "  new_state, reward, done = act(actual_state, action)\n",
        "  # print(i, action, done)\n",
        "  # Definir el nuevo estado y si ya llegó el al objetivo\n",
        "  actual_state = new_state\n",
        "  finished = done\n",
        "  i += 1\n",
        "  #total_reward = []\n",
        "  #total_reward += reward\n",
        "  # Si se llegó al objetivo imprime\n",
        "  if grid[new_state.robot_pos[0]][new_state.robot_pos[1]] == 't':\n",
        "    print(\"Lo lograste!, encontraste el tesoro!:\",new_state.robot_pos)\n",
        "    print(\"Iteraciones realizadas : \",i)\n",
        "    print(\"Cuadricula del entorno final : \")\n",
        "    for row in new_state.grid:\n",
        "      print(' '.join(row))\n",
        "    break\n",
        "  elif grid[new_state.robot_pos[0]][new_state.robot_pos[1]] == 'o':\n",
        "    print(\"El robot quedó en el obstaculo, entrenalo mejor e intenta de nuevo\")\n",
        "    print(\"Quedaste en la posición: \",new_state.robot_pos)\n",
        "    print(\"Iteraciones realizadas : \",i)\n",
        "    print(\"Cuadricula del entorno final : \")\n",
        "    for row in new_state.grid:\n",
        "      print(' '.join(row))\n",
        "    break\n",
        "  elif i== max_iter:\n",
        "   print('El algoritmo aun no converge, prueba mas iteraciones o mas episodios')\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lo lograste!, encontraste el tesoro!: [3, 3]\n",
            "Iteraciones realizadas :  6\n",
            "Cuadricula del entorno final : \n",
            "* * * o\n",
            "* o * *\n",
            "* * * o\n",
            "* o r tr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUyJnprlP3QO",
        "colab_type": "text"
      },
      "source": [
        "Vamos a explorar los cambios a partir de hiperparametros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJJr4aKmfy9Q",
        "colab_type": "text"
      },
      "source": [
        "EL CODIGO A CONTINUACIÓN ES PARA ITERAR MANUALMENTE ENTRE EPISODIOS E ITERACIONES Y COPIAR LOS RESULTADOS EN PRUEBAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXulsD7uNrMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_EPISODES = 20  #[20,50,100]   \n",
        "max_iter = 10 #[10,30,50,100]\n",
        "actual_state = start_state      # Estado incial \n",
        "\n",
        "start_state = State(grid=grid, robot_pos=[0, 0]) # Posicion inicial donde se encuentra el robot\n",
        "N_STATES = 16                                    # Posibles estados del entorno\n",
        "MAX_EPISODE_STEPS = 100                          # Número máximo de pasos por episodio\n",
        "MIN_ALPHA = 0.02                                 # Valor mínimo de la tasa de aprendizaje\n",
        "alphas = np.linspace(1.0, MIN_ALPHA, N_EPISODES) # Array decreciente de valores alfa para cada episodio\n",
        "gamma = 1.0\n",
        "eps = 0.2                                        # epsilon de balance entre exploración y explotación\n",
        "q_table = dict()                                 # Diccionario para almacenar la tabla 'q'\n",
        "\n",
        "# Exploración del entorno según los parámetros definidos\n",
        "\n",
        "for e in range(N_EPISODES):\n",
        "    \n",
        "    state = start_state\n",
        "    total_reward = 0\n",
        "    alpha = alphas[e]\n",
        "    \n",
        "    for _ in range(MAX_EPISODE_STEPS):\n",
        "        action = choose_action(state)\n",
        "        next_state, reward, done = act(state, action)\n",
        "        total_reward += reward\n",
        "        \n",
        "        q(state)[action] = q(state, action) + \\\n",
        "                alpha * (reward + gamma *  np.max(q(next_state)) - q(state, action))\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "    #print(f\"Episode {e + 1}: total reward -> {total_reward}\")\n",
        "\n",
        "#max_iter = 100                  # Definición de maximas iteraciones\n",
        "finished = False                # Parametro de parada\n",
        "actual_state = start_state      # Estado incial \n",
        "i = 0                           # Contador\n",
        "\n",
        "while  (i != max_iter or finished == True):\n",
        "  # Escoger la accion segun el estado\n",
        "  action = choose_action(actual_state)\n",
        "  # Ejecutar la accion definda y calcular la recompensa\n",
        "  new_state, reward, done = act(actual_state, action)\n",
        "  # print(i, action, done)\n",
        "  # Definir el nuevo estado y si ya llegó el al objetivo\n",
        "  actual_state = new_state\n",
        "  finished = done\n",
        "  i += 1\n",
        "  #total_reward = []\n",
        "  #total_reward += reward\n",
        "  # Si se llegó al objetivo imprime\n",
        "  if grid[new_state.robot_pos[0]][new_state.robot_pos[1]] == 't':\n",
        "    print(\"Lo lograste!, encontraste el tesoro!:\",new_state.robot_pos)\n",
        "    print(\"Iteraciones realizadas : \",i)\n",
        "    print(\"El numero maximo de iteraciones era\", max_iter)\n",
        "    print(\"Cuadricula del entorno final : \")\n",
        "    for row in new_state.grid:\n",
        "      print(' '.join(row))\n",
        "    break\n",
        "  elif grid[new_state.robot_pos[0]][new_state.robot_pos[1]] == 'o':\n",
        "    print(\"El robot quedó en el obstaculo, entrenalo mejor e intenta de nuevo\")\n",
        "    print(\"Quedaste en la posición: \",new_state.robot_pos)\n",
        "    print(\"Iteraciones realizadas : \",i)\n",
        "    print(\"Cuadricula del entorno final : \")\n",
        "    for row in new_state.grid:\n",
        "      print(' '.join(row))\n",
        "    break\n",
        "  elif i== max_iter:\n",
        "   print('El algoritmo aun no converge, prueba mas iteraciones')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}